{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Research - Phishing Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670857796595,
     "user": {
      "displayName": "Cassandra Könitzer",
      "userId": "00864874247885899119"
     },
     "user_tz": -60
    },
    "id": "AV08-ZFqDhDl",
    "outputId": "2141d8ac-0252-4d19-d657-e5fb1b47bafd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import enchant \n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned Data from CSV\n",
    "df_train = pd.read_csv(\"Datasets/cleaned/train.csv\")\n",
    "df_valid = pd.read_csv(\"Datasets/cleaned/validation.csv\")\n",
    "df_test = pd.read_csv(\"Datasets/cleaned/test.csv\")\n",
    "df_gpt = pd.read_csv(\"Datasets/cleaned/gpt.csv\")\n",
    "df = pd.read_csv(\"Datasets/cleaned/spam.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets we work with in the further steps are loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/spam2.csv\")\n",
    "df_train_roh = pd.read_csv(\"Datasets/train.csv\")\n",
    "df_test_roh = pd.read_csv(\"Datasets/test.csv\")\n",
    "df_valid_roh = pd.read_csv(\"Datasets/validation.csv\")\n",
    "df_gpt = pd.read_csv(\"gpt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step we lemmatize the dataset and remove stopwords. Therefore we use NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download (\"wordnet\")\n",
    "nltk.download (\"stopwords\")\n",
    "stopWords = set(stopwords.words('english'))\n",
    "regexp = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up old Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Columns to one uniform format\n",
    "df_train_roh = df_train_roh.rename(columns={\"sentence1\": \"text\"})\n",
    "df_valid_roh = df_valid_roh.rename(columns={\"sentence1\": \"text\"})\n",
    "df_test_roh = df_test_roh.rename(columns={\"sentence1\": \"text\"})\n",
    "df = df.rename(columns={\"v2\": \"text\", \"v1\": \"label\"})\n",
    "\n",
    "# Replace Labels\n",
    "df.label = df.label.str.replace(\"ham\", \"normal\")\n",
    "\n",
    "# Drop NaN values\n",
    "df = df.loc[np.logical_and(df.label.notnull(), df.text.notnull())]\n",
    "df_train_roh = df_train_roh.loc[np.logical_and(df_train_roh.label.notnull(), df_train_roh.text.notnull())]\n",
    "df_valid_roh = df_valid_roh.loc[np.logical_and(df_valid_roh.label.notnull(), df_valid_roh.text.notnull())]\n",
    "df_test_roh = df_test_roh.loc[np.logical_and(df_test_roh.label.notnull(), df_test_roh.text.notnull())]\n",
    "\n",
    "# Drop Columns\n",
    "df_train_roh.drop(\"id\", axis=1, inplace=True)\n",
    "df_valid_roh.drop(\"id\", axis=1, inplace=True)\n",
    "df_test_roh.drop(\"id\", axis=1, inplace=True)\n",
    "df = df[[\"text\", \"label\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GPT generated test Mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our data to train our model, we go through typical preprocessing steps. Therefore we create a lemmatize function, which we can apply on the different datasets. In this part the data is also upsampled, so that tere is the same amount of spam mails and normal mails in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        if detect(text) == \"en\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def Lemmatize(x):\n",
    "    x = regexp.tokenize(x)\n",
    "    text = \"\"\n",
    "    for i in x:\n",
    "        if i not in stopWords and dic.check(i):\n",
    "            #lemm = wnl.lemmatize(i)\n",
    "            text += i + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten mit label \"normal\" herausfiltern\n",
    "df_train_normal = df_train_roh.loc[df_train_roh.label == \"normal\"].copy()\n",
    "df_valid_normal = df_valid_roh.loc[df_valid_roh.label == \"normal\"].copy()\n",
    "df_test_normal = df_test_roh.loc[df_test_roh.label == \"normal\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selbe Menge an Daten mit label \"spam\" herausfiltern\n",
    "df_train_spam = df_train_roh.loc[df_train_roh.label == \"spam\"].sample(len(df_train_normal))\n",
    "df_valid_spam = df_valid_roh.loc[df_valid_roh.label == \"spam\"].sample(len(df_valid_normal))\n",
    "df_test_spam = df_test_roh.loc[df_test_roh.label == \"spam\"].sample(len(df_test_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten zusammenführen\n",
    "df_train = pd.concat([df_test_normal, df_train_spam])\n",
    "df_valid = pd.concat([df_valid_normal, df_valid_spam])\n",
    "df_test = pd.concat([df_train_normal, df_test_spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten mischen\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_valid = df_valid.sample(frac=1)\n",
    "df_test = df_test.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten vorbereiten\n",
    "df_train[\"text\"] = df_train.text.apply(Lemmatize)\n",
    "df_valid[\"text\"] = df_valid.text.apply(Lemmatize)\n",
    "df_test[\"text\"] = df_test.text.apply(Lemmatize)\n",
    "df_gpt.text = df_gpt.text.apply(Lemmatize)\n",
    "df.text = df.text.apply(Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"english\"] = df_train.text.apply(is_english)\n",
    "df_test[\"english\"] = df_test.text.apply(is_english)\n",
    "df[\"english\"] = df.text.apply(is_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.english == 1]\n",
    "df_test = df_test.loc[df_test.english == 1]\n",
    "df = df.loc[df.english == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps we prepare the data of the train and test dataset. The data comes from one dataset, which we split into training and test data. Therefore we rename the columns, remove a column named \"id\", that is not important for the next steps, and check if the datasets contains data that is not available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We searched for further features, except of the text itself, that we can use to train the model. Therefore we create to functions to see if the text contains links and ip adresses. With the following function we check if the the texts contain links. We want to create a new column with this information, so we can use it to train the model later on. We also create a column with the lenght of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim-h\\AppData\\Local\\Temp\\ipykernel_13276\\3454260033.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['contains_link'] = df_train['text'].apply(containslink)\n"
     ]
    }
   ],
   "source": [
    "def containslink(text):\n",
    "  pattern = r\"(http|ftp|https)://([\\w-]+(?:(?:.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
    "  return int(bool(re.search(pattern, text)))\n",
    "\n",
    "df_train['contains_link'] = df_train['text'].apply(containslink)\n",
    "df_test['contains_link'] = df_test['text'].apply(containslink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim-h\\AppData\\Local\\Temp\\ipykernel_13276\\2596868674.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['contains_ip'] = df_train['text'].apply(containsip)\n"
     ]
    }
   ],
   "source": [
    "def containsip(text):\n",
    "  pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n",
    "  return int(bool(re.search(pattern, text)))\n",
    "\n",
    "df_train['contains_ip'] = df_train['text'].apply(containsip)\n",
    "df_test['contains_ip'] = df_test['text'].apply(containsip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim-h\\AppData\\Local\\Temp\\ipykernel_13276\\4249468257.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['length'] = df_train['text'].apply(len)\n"
     ]
    }
   ],
   "source": [
    "df_train['length'] = df_train['text'].apply(len)\n",
    "df_test['length'] = df_test['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>english</th>\n",
       "      <th>contains_link</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free shipping jewelry accessories orders today...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U Room may anything Chad anyone would somethin...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fantastic luxury items less half price Want Di...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dreams achievable websites like let enjoy life...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>place Well standing She engineered teeth detai...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>PUBLIC 1 0 Transitional EN org TR transitional...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>We help recharge health skull</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>Any man last 40 minutes MEN S JOURNAL Health P...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>M hate It decision entirely blasphemy meriting...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>Get ultra luxurious watches fraction original ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1216 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label  english  \\\n",
       "0     Free shipping jewelry accessories orders today...  spam        1   \n",
       "2     U Room may anything Chad anyone would somethin...  spam        1   \n",
       "3     Fantastic luxury items less half price Want Di...  spam        1   \n",
       "4     Dreams achievable websites like let enjoy life...  spam        1   \n",
       "5     place Well standing She engineered teeth detai...  spam        1   \n",
       "...                                                 ...   ...      ...   \n",
       "1503  PUBLIC 1 0 Transitional EN org TR transitional...  spam        1   \n",
       "1504                     We help recharge health skull   spam        1   \n",
       "1505  Any man last 40 minutes MEN S JOURNAL Health P...  spam        1   \n",
       "1506  M hate It decision entirely blasphemy meriting...  spam        1   \n",
       "1507  Get ultra luxurious watches fraction original ...  spam        1   \n",
       "\n",
       "      contains_link  contains_ip  length  \n",
       "0                 0            0      66  \n",
       "2                 0            0     827  \n",
       "3                 0            0      59  \n",
       "4                 0            0      76  \n",
       "5                 0            0    1416  \n",
       "...             ...          ...     ...  \n",
       "1503              0            0    6192  \n",
       "1504              0            0      30  \n",
       "1505              0            0    1969  \n",
       "1506              0            0    2885  \n",
       "1507              0            0      72  \n",
       "\n",
       "[1216 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "error",
     "timestamp": 1670858096218,
     "user": {
      "displayName": "Cassandra Könitzer",
      "userId": "00864874247885899119"
     },
     "user_tz": -60
    },
    "id": "o1_26WFI4Nzw",
    "outputId": "dece3f33-3017-46a9-f5f7-b0f29377ddd1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df.text)\n",
    "\n",
    "model = svm.SVC()\n",
    "model.fit(features, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the model detect our testMail.txt as a spam mail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"testMail.txt\", \"r\")\n",
    "\n",
    "features_test = cv.transform(f)\n",
    "# print(model.score(features_test,y_test))\n",
    "print(model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9967695620961953\n"
     ]
    }
   ],
   "source": [
    "features_test = cv.transform(df.text)\n",
    "print(model.score(features_test,df.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model, that we trained with the spam2 dataset, has an accuracy of 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:10000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(degree=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(degree=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(degree=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(degree=1)\n",
    "model.fit(features,df_train.iloc[0:10000].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987\n"
     ]
    }
   ],
   "source": [
    "features_test = cv.transform(df_test.iloc[0:10000].text)\n",
    "print(model.score(features_test,df_test.iloc[0:10000].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model with the test dataset and get an accuracy of 0.98."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with the test data to compare if it has a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04664433451485997\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.loc[np.logical_and(df_test.label.notnull(), df_test.text.notnull())]\n",
    "\n",
    "features_test = cv.transform(df_test.text)\n",
    "print(model.score(features_test,df_test.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testmail is now detected as spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"testMail.txt\", \"r\")\n",
    "\n",
    "features_test = cv.transform(f)\n",
    "# print(model.score(features_test,y_test))\n",
    "print(model.predict(features_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit our the test and training dataset by using only the first 2000 lines for training because of the size of the dataset and therefore resulting performance issues. \n",
    "We train the model with the **train** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[np.logical_and(df_train.label.notnull(), df_train.text.notnull())]\n",
    "df_test = df_test.loc[np.logical_and(df_test.label.notnull(), df_test.text.notnull())]\n",
    "df_valid = df_valid.loc[np.logical_and(df_valid.label.notnull(), df_valid.text.notnull())]\n",
    "df_gpt = df_gpt.loc[np.logical_and(df_gpt.label.notnull(), df_gpt.text.notnull())]\n",
    "df = df.loc[np.logical_and(df.label.notnull(), df.text.notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:3500].text).toarray()\n",
    "valid_features = cv.transform(df_valid.iloc[0:3500].text).toarray()\n",
    "test_features = cv.transform(df_test.iloc[0:3500].text).toarray()\n",
    "gpt_features = cv.transform(df_gpt.iloc[0:3500].text).toarray()\n",
    "df_features = cv.transform(df.iloc[0:3500].text).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631428571428572"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(feat, df_test.iloc[:3500].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 2240 points : 85\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(valid_features)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (valid_features.shape[0], (df_valid.iloc[:3500].label != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-Analyse mit Linearer Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:2000].text).toarray()\n",
    "test_features = cv.transform(df_test.iloc[0:2000].text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_numerical(label):\n",
    "    if label == \"spam\":\n",
    "        return 0\n",
    "    if label == \"normal\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "label = df_train.iloc[:2000].label.apply(label_to_numerical)\n",
    "test_label = df_test.iloc[:2000].label.apply(label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0893262944415882"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LogisticRegression().fit(features, label)\n",
    "reg.score(test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(test_features, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineare Regression ohne Text-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_numerical(label):\n",
    "    if label == \"spam\":\n",
    "        return 0\n",
    "    if label == \"normal\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "data = df_train[[\"contains_link\", \"contains_ip\", \"length\"]].iloc[:2000]\n",
    "test_data = df_test[[\"contains_link\", \"contains_ip\", \"length\"]].iloc[:2000]\n",
    "label = df_train.iloc[:2000].label.apply(label_to_numerical)\n",
    "test_label = df_test.iloc[:2000].label.apply(label_to_numerical)\n",
    "data[\"prediction\"] =  reg.predict(features)\n",
    "test_data[\"prediction\"] = reg.predict(test_features)\n",
    "\n",
    "reg2 = LinearRegression().fit(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.628608423279085"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.score(test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.text)\n",
    "test_features = cv.transform(df_test.text)\n",
    "spam2_features = cv.transform((df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DcsTree = DecisionTreeClassifier()\n",
    "DcsTree.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DcsTree.score(spam2_features, df.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.text)\n",
    "test_features = cv.transform(df_test.text)\n",
    "spam2_features = cv.transform((df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndFrst = RandomForestClassifier()\n",
    "RndFrst.fit(features, df_train.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test mit selben Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913333333333333"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RndFrst.score(test_features, df_test.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test mit anderem Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15466666666666667"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RndFrst.score(spam2_features, df.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'min_samples_leaf': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters you want to search over\n",
    "param_grid = {'max_depth': [1, 2, 3, 4, 5],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "              'splitter': [\"best\", \"random\"]}\n",
    "\n",
    "# Create a decision tree model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(features, df_train.label)\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DcsTree = DecisionTreeClassifier(max_depth = 1, min_samples_leaf = 1, splitter = 'best')\n",
    "DcsTree.fit(features, df_train.label)\n",
    "print(f\"Score mit Testdaten aus dem selben Dataset: {DcsTree.score(test_features, df_test.label)}\")\n",
    "print(f\"Score mit Testdaten aus dem alten Dataset: {DcsTree.score(spam2_features, df.label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters you want to search over\n",
    "param_grid = {\n",
    "                'max_depth': [1, 2, 3, 4, 5], \n",
    "                'min_samples_leaf': [1, 2, 3, 4, 5],}\n",
    "\n",
    "# Create a decision tree model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(features, df_train.label)\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RndFrst = RandomForestClassifier(max_depth = 1, min_samples_leaf = 1)\n",
    "RndFrst.fit(features, df_train.label)\n",
    "print(f\"Score mit Testdaten aus dem selben Dataset: {RndFrst.score(test_features, df_test.label)}\")\n",
    "print(f\"Score mit Testdaten aus dem alten Dataset: {RndFrst.score(spam2_features, df.label)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:3500].text).toarray()\n",
    "labels = df_train.iloc[0:3500].label.factorize()[0]\n",
    "valid_features = cv.transform(df_valid.iloc[0:3500].text).toarray()\n",
    "valid_labels = df_valid.iloc[0:3500].label.factorize()[0]\n",
    "test_features = cv.transform(df_test.iloc[0:3500].text).toarray()\n",
    "test_labels = df_test.iloc[0:3500].label.factorize()[0]\n",
    "gpt_features = cv.transform(df_gpt.iloc[0:3500].text).toarray()\n",
    "gpt_labels = df_gpt.iloc[0:3500].label.factorize()[0]\n",
    "df_features = cv.transform(df.iloc[0:3500].text).toarray()\n",
    "df_labels = df.iloc[0:3500].label.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the data\n",
    "data = pd.concat([df_train, df_test, df_valid, df, df_gpt])\n",
    "\n",
    "# Mix data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split data into train, validation and test\n",
    "data_train, data_test = train_test_split(data, test_size=0.1)\n",
    "data_train, data_valid = train_test_split(data_train, test_size=0.5)\n",
    "\n",
    "# Vectorize the data\n",
    "cv = CountVectorizer()\n",
    "cv.fit(data.text)\n",
    "features = cv.transform(data_train.text).toarray()\n",
    "test_features = cv.transform(data_test.text).toarray()\n",
    "valid_features = cv.transform(data_valid.text).toarray()\n",
    "\n",
    "# Factorize the labels\n",
    "labels = data_train.label.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "test_labels = data_test.label.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "valid_labels = data_valid.label.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "# Cut training and validation data to the same length\n",
    "samples = features.shape[0] - 10\n",
    "features = features[:samples]\n",
    "labels = labels[:samples]\n",
    "valid_features = valid_features[:samples]\n",
    "valid_labels = valid_labels[:samples]\n",
    "\n",
    "# transform old data \n",
    "df_features = cv.transform(df.text).toarray()\n",
    "gpt_features = cv.transform(df_gpt.text).toarray()\n",
    "old_test_features = cv.transform(df_test.text).toarray()\n",
    "\n",
    "# Vectorize old labels\n",
    "df_labels = df.label.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "gpt_labels = df_gpt.label.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "old_test_labels = df_test.label.apply(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(features, labels, valid_features, valid_labels):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=(features.shape[1],))\n",
    "\n",
    "    # Define the hidden layers\n",
    "    hidden_layer = Dense(units=256, activation='relu')(input_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(units=128, activation='relu')(hidden_layer)\n",
    "\n",
    "    # Define the output layer\n",
    "    output_layer = Dense(units=1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(features, labels, validation_data=[valid_features, valid_labels], epochs=6, batch_size=32)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "312/312 [==============================] - 17s 54ms/step - loss: 0.1740 - accuracy: 0.9402 - val_loss: 0.1047 - val_accuracy: 0.9654\n",
      "Epoch 2/6\n",
      "312/312 [==============================] - 18s 59ms/step - loss: 0.0643 - accuracy: 0.9843 - val_loss: 0.1554 - val_accuracy: 0.9663\n",
      "Epoch 3/6\n",
      "312/312 [==============================] - 17s 54ms/step - loss: 0.0242 - accuracy: 0.9922 - val_loss: 0.1217 - val_accuracy: 0.9658\n",
      "Epoch 4/6\n",
      "312/312 [==============================] - 16s 53ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.1337 - val_accuracy: 0.9685\n",
      "Epoch 5/6\n",
      "312/312 [==============================] - 17s 53ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.1445 - val_accuracy: 0.9664\n",
      "Epoch 6/6\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 0.0043 - accuracy: 0.9979 - val_loss: 0.1590 - val_accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "model = train_network(features, labels, valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 2s 7ms/step - loss: 0.1590 - accuracy: 0.9649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15895545482635498, 0.9648559093475342]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 1s 7ms/step - loss: 0.1865 - accuracy: 0.9607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18650828301906586, 0.9607400894165039]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 1s 8ms/step - loss: 0.1334 - accuracy: 0.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13335511088371277, 0.9609755873680115]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df_features, df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 8ms/step - loss: 0.0646 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06460397690534592, 0.9885714054107666]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(gpt_features, gpt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 1s 8ms/step - loss: 0.0859 - accuracy: 0.9856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08588790893554688, 0.9856407642364502]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(old_test_features, old_test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with manually written Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"Dear Customer, we are happy to inform you that you have won an iPhone 17 in our annual customer lottery. Please click on the link to claim your price. Warning: the link will expire within 7 Days and so will your price. \",\n",
    "        \"Warning, your bank account has been compromised. It seems like your account has been hacked. Click on the link to prevent your account from being deactivated.\",\n",
    "        \"Attention last warning! The bill with the number 123 is still open. Please pay the invoice within 7 days. Otherwise, we will be forced to take legal action.\",\n",
    "        \"Viagra without prescription. Buy Viagra without a doctor's prescription on our website. We are the market leader for sexual enhancers. Buy cheap and effective medicine now.\",\n",
    "        \"Dear Peter, I am writing to you regarding the appointment on Thursday. Unfortunately, I can only in the afternoon. Would you mind postponing the date?\",\n",
    "        \"Dear Mr. Soundso, as we already discussed during our last conversation, the delivery on Thursday can take place as planned. Please send me the exact address and your phone number.\",\n",
    "        \"Dear customer, enclosed you will find the invoice for your order. Please note that the invoice has already been paid. She is only for her records.\",\n",
    "        \"Students Attention, this semester our summer camp takes place again. You can expect 3 weeks of hard work in the labour camp. The course can be credited with half an ECTS.\"\n",
    "        ],\n",
    "    \"label\":[\"spam\", \"spam\", \"spam\", \"spam\", \"normal\", \"normal\", \"normal\", \"normal\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_features = cv.transform(df_man.text).toarray()\n",
    "man_labels = df_man.label.apply(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3967 - accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3966676592826843, 0.875]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(man_features, man_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear Customer, we are happy to inform you that...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Warning, your bank account has been compromise...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attention last warning! The bill with the numb...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Viagra without prescription. Buy Viagra withou...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Peter, I am writing to you regarding the ...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dear Mr. Soundso, as we already discussed duri...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dear customer, enclosed you will find the invo...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Students Attention, this semester our summer c...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   label\n",
       "0  Dear Customer, we are happy to inform you that...    spam\n",
       "1  Warning, your bank account has been compromise...    spam\n",
       "2  Attention last warning! The bill with the numb...    spam\n",
       "3  Viagra without prescription. Buy Viagra withou...    spam\n",
       "4  Dear Peter, I am writing to you regarding the ...  normal\n",
       "5  Dear Mr. Soundso, as we already discussed duri...  normal\n",
       "6  Dear customer, enclosed you will find the invo...  normal\n",
       "7  Students Attention, this semester our summer c...  normal"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series([\"Dear Mr. Soundso, as we already discussed during our last conversation, the delivery on Thursday can take place as planned. Please send me the exact address and your phone number\"])\n",
    "test = cv.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test)[0][0] > 0.5"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5N+ShycLTgI0CJ1yYU2gG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0379c67db9d5adaac49bb272aceb784bcefbb89b24811ac05cf259bf8ea8bf96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
