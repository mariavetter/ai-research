{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Research - Phishing Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670857796595,
     "user": {
      "displayName": "Cassandra Könitzer",
      "userId": "00864874247885899119"
     },
     "user_tz": -60
    },
    "id": "AV08-ZFqDhDl",
    "outputId": "2141d8ac-0252-4d19-d657-e5fb1b47bafd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import enchant \n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned Data from CSV\n",
    "df_train = pd.read_csv(\"Datasets/cleaned/train.csv\")\n",
    "df_valid = pd.read_csv(\"Datasets/cleaned/validation.csv\")\n",
    "df_test = pd.read_csv(\"Datasets/cleaned/test.csv\")\n",
    "df_gpt = pd.read_csv(\"Datasets/cleaned/gpt.csv\")\n",
    "df = pd.read_csv(\"Datasets/cleaned/spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets we work with in the further steps are read in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/spam2.csv\")\n",
    "df_train_roh = pd.read_csv(\"Datasets/train.csv\")\n",
    "df_test_roh = pd.read_csv(\"Datasets/test.csv\")\n",
    "df_valid_roh = pd.read_csv(\"Datasets/validation.csv\")\n",
    "df_gpt = pd.read_csv(\"df_gpt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step we lemmatize the dataset and remove stopwords. Therefore we use NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download (\"wordnet\")\n",
    "nltk.download (\"stopwords\")\n",
    "stopWords = set(stopwords.words('english'))\n",
    "regexp = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up old Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Columns to one uniform format\n",
    "df_train_roh = df_train_roh.rename(columns={\"sentence1\": \"text\"})\n",
    "df_valid_roh = df_valid_roh.rename(columns={\"sentence1\": \"text\"})\n",
    "df_test_roh = df_test_roh.rename(columns={\"sentence1\": \"text\"})\n",
    "df = df.rename(columns={\"v2\": \"text\", \"v1\": \"label\"})\n",
    "\n",
    "# Replace Labels\n",
    "df.label = df.label.str.replace(\"ham\", \"normal\")\n",
    "\n",
    "# Drop NaN values\n",
    "df = df.loc[np.logical_and(df.label.notnull(), df.text.notnull())]\n",
    "df_train_roh = df_train_roh.loc[np.logical_and(df_train_roh.label.notnull(), df_train_roh.text.notnull())]\n",
    "df_valid_roh = df_valid_roh.loc[np.logical_and(df_valid_roh.label.notnull(), df_valid_roh.text.notnull())]\n",
    "df_test_roh = df_test_roh.loc[np.logical_and(df_test_roh.label.notnull(), df_test_roh.text.notnull())]\n",
    "\n",
    "# Drop Columns\n",
    "df_train_roh.drop(\"id\", axis=1, inplace=True)\n",
    "df_valid_roh.drop(\"id\", axis=1, inplace=True)\n",
    "df_test_roh.drop(\"id\", axis=1, inplace=True)\n",
    "df = df[[\"text\", \"label\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GPT generated test Mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our data to train our model, we go through typical preprocessing steps. Therefore we create a lemmatize function, which we can apply on the different datasets. In this part the data is also upsampled, so that tere is the same amount of spam mails and normal mails in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        if detect(text) == \"en\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def Lemmatize(x):\n",
    "    x = regexp.tokenize(x)\n",
    "    text = \"\"\n",
    "    for i in x:\n",
    "        if i not in stopWords and dic.check(i):\n",
    "            #lemm = wnl.lemmatize(i)\n",
    "            text += i + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten mit label \"normal\" herausfiltern\n",
    "df_train_normal = df_train_roh.loc[df_train_roh.label == \"normal\"].copy()\n",
    "df_valid_normal = df_valid_roh.loc[df_valid_roh.label == \"normal\"].copy()\n",
    "df_test_normal = df_test_roh.loc[df_test_roh.label == \"normal\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selbe Menge an Daten mit label \"spam\" herausfiltern\n",
    "df_train_spam = df_train_roh.loc[df_train_roh.label == \"spam\"].sample(len(df_train_normal))\n",
    "df_valid_spam = df_valid_roh.loc[df_valid_roh.label == \"spam\"].sample(len(df_valid_normal))\n",
    "df_test_spam = df_test_roh.loc[df_test_roh.label == \"spam\"].sample(len(df_test_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten zusammenführen\n",
    "df_train = pd.concat([df_test_normal, df_train_spam])\n",
    "df_valid = pd.concat([df_valid_normal, df_valid_spam])\n",
    "df_test = pd.concat([df_train_normal, df_test_spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten mischen\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_valid = df_valid.sample(frac=1)\n",
    "df_test = df_test.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten vorbereiten\n",
    "df_train[\"text\"] = df_train.text.apply(Lemmatize)\n",
    "df_valid[\"text\"] = df_valid.text.apply(Lemmatize)\n",
    "df_test[\"text\"] = df_test.text.apply(Lemmatize)\n",
    "df_gpt.text = df_gpt.text.apply(Lemmatize)\n",
    "df.text = df.text.apply(Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"english\"] = df_train.text.apply(is_english)\n",
    "df_test[\"english\"] = df_test.text.apply(is_english)\n",
    "df[\"english\"] = df.text.apply(is_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.english == 1]\n",
    "df_test = df_test.loc[df_test.english == 1]\n",
    "df = df.loc[df.english == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps we prepare the data of the train and test dataset. The data comes from one dataset, which we split into training and test data. Therefore we rename the columns, remove a column named \"id\", that is not important for the next steps, and check if the datasets contains data that is not available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We searched for further features, except of the text itself, that we can use to train the model. Therefore we create to functions to see if the text contains links and ip adresses. With the following function we check if the the texts contain links. We want to create a new column with this information, so we can use it to train the model later on. We also create a column with the lenght of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim-h\\AppData\\Local\\Temp\\ipykernel_13276\\3454260033.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['contains_link'] = df_train['text'].apply(containslink)\n"
     ]
    }
   ],
   "source": [
    "def containslink(text):\n",
    "  pattern = r\"(http|ftp|https)://([\\w-]+(?:(?:.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
    "  return int(bool(re.search(pattern, text)))\n",
    "\n",
    "df_train['contains_link'] = df_train['text'].apply(containslink)\n",
    "df_test['contains_link'] = df_test['text'].apply(containslink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim-h\\AppData\\Local\\Temp\\ipykernel_13276\\2596868674.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['contains_ip'] = df_train['text'].apply(containsip)\n"
     ]
    }
   ],
   "source": [
    "def containsip(text):\n",
    "  pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n",
    "  return int(bool(re.search(pattern, text)))\n",
    "\n",
    "df_train['contains_ip'] = df_train['text'].apply(containsip)\n",
    "df_test['contains_ip'] = df_test['text'].apply(containsip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim-h\\AppData\\Local\\Temp\\ipykernel_13276\\4249468257.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['length'] = df_train['text'].apply(len)\n"
     ]
    }
   ],
   "source": [
    "df_train['length'] = df_train['text'].apply(len)\n",
    "df_test['length'] = df_test['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>english</th>\n",
       "      <th>contains_link</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free shipping jewelry accessories orders today...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U Room may anything Chad anyone would somethin...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fantastic luxury items less half price Want Di...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dreams achievable websites like let enjoy life...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>place Well standing She engineered teeth detai...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>PUBLIC 1 0 Transitional EN org TR transitional...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>We help recharge health skull</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>Any man last 40 minutes MEN S JOURNAL Health P...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>M hate It decision entirely blasphemy meriting...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>Get ultra luxurious watches fraction original ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1216 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label  english  \\\n",
       "0     Free shipping jewelry accessories orders today...  spam        1   \n",
       "2     U Room may anything Chad anyone would somethin...  spam        1   \n",
       "3     Fantastic luxury items less half price Want Di...  spam        1   \n",
       "4     Dreams achievable websites like let enjoy life...  spam        1   \n",
       "5     place Well standing She engineered teeth detai...  spam        1   \n",
       "...                                                 ...   ...      ...   \n",
       "1503  PUBLIC 1 0 Transitional EN org TR transitional...  spam        1   \n",
       "1504                     We help recharge health skull   spam        1   \n",
       "1505  Any man last 40 minutes MEN S JOURNAL Health P...  spam        1   \n",
       "1506  M hate It decision entirely blasphemy meriting...  spam        1   \n",
       "1507  Get ultra luxurious watches fraction original ...  spam        1   \n",
       "\n",
       "      contains_link  contains_ip  length  \n",
       "0                 0            0      66  \n",
       "2                 0            0     827  \n",
       "3                 0            0      59  \n",
       "4                 0            0      76  \n",
       "5                 0            0    1416  \n",
       "...             ...          ...     ...  \n",
       "1503              0            0    6192  \n",
       "1504              0            0      30  \n",
       "1505              0            0    1969  \n",
       "1506              0            0    2885  \n",
       "1507              0            0      72  \n",
       "\n",
       "[1216 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "error",
     "timestamp": 1670858096218,
     "user": {
      "displayName": "Cassandra Könitzer",
      "userId": "00864874247885899119"
     },
     "user_tz": -60
    },
    "id": "o1_26WFI4Nzw",
    "outputId": "dece3f33-3017-46a9-f5f7-b0f29377ddd1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df.text)\n",
    "\n",
    "model = svm.SVC()\n",
    "model.fit(features, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the model detect our testMail.txt as a spam mail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"testMail.txt\", \"r\")\n",
    "\n",
    "features_test = cv.transform(f)\n",
    "# print(model.score(features_test,y_test))\n",
    "print(model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9967695620961953\n"
     ]
    }
   ],
   "source": [
    "features_test = cv.transform(df.text)\n",
    "print(model.score(features_test,df.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model, that we trained with the spam2 dataset, has an accuracy of 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:10000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(degree=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(degree=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(degree=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(degree=1)\n",
    "model.fit(features,df_train.iloc[0:10000].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987\n"
     ]
    }
   ],
   "source": [
    "features_test = cv.transform(df_test.iloc[0:10000].text)\n",
    "print(model.score(features_test,df_test.iloc[0:10000].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model with the test dataset and get an accuracy of 0.98."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with the test data to compare if it has a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04664433451485997\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.loc[np.logical_and(df_test.label.notnull(), df_test.text.notnull())]\n",
    "\n",
    "features_test = cv.transform(df_test.text)\n",
    "print(model.score(features_test,df_test.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testmail is now detected as spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"testMail.txt\", \"r\")\n",
    "\n",
    "features_test = cv.transform(f)\n",
    "# print(model.score(features_test,y_test))\n",
    "print(model.predict(features_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit our the test and training dataset by using only the first 2000 lines for training because of the size of the dataset and therefore resulting performance issues. \n",
    "We train the model with the **train** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[np.logical_and(df_train.label.notnull(), df_train.text.notnull())]\n",
    "df_test = df_test.loc[np.logical_and(df_test.label.notnull(), df_test.text.notnull())]\n",
    "df_valid = df_valid.loc[np.logical_and(df_valid.label.notnull(), df_valid.text.notnull())]\n",
    "df_gpt = df_gpt.loc[np.logical_and(df_gpt.label.notnull(), df_gpt.text.notnull())]\n",
    "df = df.loc[np.logical_and(df.label.notnull(), df.text.notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:3500].text).toarray()\n",
    "valid_features = cv.transform(df_valid.iloc[0:3500].text).toarray()\n",
    "test_features = cv.transform(df_test.iloc[0:3500].text).toarray()\n",
    "gpt_features = cv.transform(df_gpt.iloc[0:3500].text).toarray()\n",
    "df_features = cv.transform(df.iloc[0:3500].text).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631428571428572"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(feat, df_test.iloc[:3500].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 2240 points : 85\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(valid_features)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (valid_features.shape[0], (df_valid.iloc[:3500].label != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also train the model with the **spam2** dataset to compare the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7040684115055714"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = df2\n",
    "test_features = cv.transform(test_sample.text).toarray()\n",
    "\n",
    "gnb.score(test_features, test_sample.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text             Free shipping on all jewelry and accessories o...\n",
       "label                                                         spam\n",
       "contains_link                                                False\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testen mit Daten von GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_features = cv.transform(df_gpt.text).toarray()\n",
    "gnb.score(gpt_features, df_gpt.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(gpt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt[\"outcome\"] = df_gpt.label == y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear Name n know could earning money TODAY We ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear customer selected participate exclusive o...</td>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey customer amazing new product absolutely ne...</td>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear friend selected get free luxury watch Cli...</td>\n",
       "      <td>spam</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey special offer right want miss Click link f...</td>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi noticed missing tons amazing opportunities ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n Valued Customer exclusive offer Claim free g...</td>\n",
       "      <td>spam</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Greetings chosen receive Amazing offer Visit w...</td>\n",
       "      <td>spam</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hey proud owner brand new Rolex watch without ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hey friend dreams come true lifetime offer Get...</td>\n",
       "      <td>spam</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hi Everyone n hosting game night Friday house ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hi John n hope mail finds well I wanted reach ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hi Team n sending mail whole team remind every...</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hi Tom n wanted check received email I sent re...</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hi There n writing inform new internship oppor...</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hello Mary n friendly reminder appointment tom...</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hi Everyone n wanted let know I updated projec...</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hi Bill n checking make sure received document...</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hi Everyone n join us company picnic Saturday ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   label  outcome\n",
       "0   Dear Name n know could earning money TODAY We ...    spam    False\n",
       "1   Dear customer selected participate exclusive o...    spam    False\n",
       "2   Hey customer amazing new product absolutely ne...    spam    False\n",
       "3   Dear friend selected get free luxury watch Cli...    spam     True\n",
       "4   Hey special offer right want miss Click link f...    spam    False\n",
       "5   Hi noticed missing tons amazing opportunities ...    spam    False\n",
       "6   n Valued Customer exclusive offer Claim free g...    spam     True\n",
       "7   Greetings chosen receive Amazing offer Visit w...    spam     True\n",
       "8   Hey proud owner brand new Rolex watch without ...    spam     True\n",
       "9   Hey friend dreams come true lifetime offer Get...    spam     True\n",
       "10  Hi Everyone n hosting game night Friday house ...  normal    False\n",
       "11  Hi John n hope mail finds well I wanted reach ...  normal     True\n",
       "12  Hi Team n sending mail whole team remind every...  normal    False\n",
       "13  Hi Tom n wanted check received email I sent re...  normal     True\n",
       "14  Hi There n writing inform new internship oppor...  normal     True\n",
       "15  Hello Mary n friendly reminder appointment tom...  normal     True\n",
       "16  Hi Everyone n wanted let know I updated projec...  normal     True\n",
       "17  Hi Bill n checking make sure received document...  normal     True\n",
       "18  Hi Everyone n join us company picnic Saturday ...  normal    False"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 1.35784265e-18 7.11283512e-26]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "# Create a GaussianNB classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the classifier to the data\n",
    "gnb.fit(X, y)\n",
    "\n",
    "# Get the predicted probabilities for the first sample\n",
    "probs = gnb.predict_proba([X[0]])[0]\n",
    "\n",
    "# Print the predicted probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-Analyse mit Linearer Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:2000].text).toarray()\n",
    "test_features = cv.transform(df_test.iloc[0:2000].text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_numerical(label):\n",
    "    if label == \"spam\":\n",
    "        return 0\n",
    "    if label == \"normal\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "label = df_train.iloc[:2000].label.apply(label_to_numerical)\n",
    "test_label = df_test.iloc[:2000].label.apply(label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0893262944415882"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LogisticRegression().fit(features, label)\n",
    "reg.score(test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(test_features, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineare Regression ohne Text-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_numerical(label):\n",
    "    if label == \"spam\":\n",
    "        return 0\n",
    "    if label == \"normal\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "data = df_train[[\"contains_link\", \"contains_ip\", \"length\"]].iloc[:2000]\n",
    "test_data = df_test[[\"contains_link\", \"contains_ip\", \"length\"]].iloc[:2000]\n",
    "label = df_train.iloc[:2000].label.apply(label_to_numerical)\n",
    "test_label = df_test.iloc[:2000].label.apply(label_to_numerical)\n",
    "data[\"prediction\"] =  reg.predict(features)\n",
    "test_data[\"prediction\"] = reg.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = LinearRegression().fit(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.628608423279085"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.score(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data[\"prediction\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2.score(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_reg = LogisticRegression().fit(data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test mit alten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = cv.transform(df.iloc[0:2000].text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_numerical(label):\n",
    "    if label == \"spam\":\n",
    "        return 0\n",
    "    if label == \"normal\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "test_label = df.iloc[:2000].label.apply(label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.149"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Logistic_reg.score(test_data, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.text)\n",
    "test_features = cv.transform(df_test.text)\n",
    "spam2_features = cv.transform((df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DcsTree = DecisionTreeClassifier()\n",
    "DcsTree.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15466666666666667"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DcsTree.score(spam2_features, df.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.text)\n",
    "test_features = cv.transform(df_test.text)\n",
    "spam2_features = cv.transform((df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RndFrst = RandomForestClassifier()\n",
    "RndFrst.fit(features, df_train.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test mit selben Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913333333333333"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RndFrst.score(test_features, df_test.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test mit anderem Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15466666666666667"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RndFrst.score(spam2_features, df.label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest mit Gridsearch Fine-Tunen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'min_samples_leaf': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters you want to search over\n",
    "param_grid = {'max_depth': [1, 2, 3, 4, 5],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "              'splitter': [\"best\", \"random\"]}\n",
    "\n",
    "# Create a decision tree model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(features, df_train.label)\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mit Testdaten aus dem selben Dataset: 0.994\n",
      "Score mit Testdaten aus dem alten Dataset: 0.15466666666666667\n"
     ]
    }
   ],
   "source": [
    "DcsTree = DecisionTreeClassifier(max_depth = 1, min_samples_leaf = 1, splitter = 'best')\n",
    "DcsTree.fit(features, df_train.label)\n",
    "print(f\"Score mit Testdaten aus dem selben Dataset: {DcsTree.score(test_features, df_test.label)}\")\n",
    "print(f\"Score mit Testdaten aus dem alten Dataset: {DcsTree.score(spam2_features, df.label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters you want to search over\n",
    "param_grid = {\n",
    "                'max_depth': [1, 2, 3, 4, 5], \n",
    "                'min_samples_leaf': [1, 2, 3, 4, 5],}\n",
    "\n",
    "# Create a decision tree model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(features, df_train.label)\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mit Testdaten aus dem selben Dataset: 0.986\n",
      "Score mit Testdaten aus dem alten Dataset: 0.15466666666666667\n"
     ]
    }
   ],
   "source": [
    "RndFrst = RandomForestClassifier(max_depth = 1, min_samples_leaf = 1)\n",
    "RndFrst.fit(features, df_train.label)\n",
    "print(f\"Score mit Testdaten aus dem selben Dataset: {RndFrst.score(test_features, df_test.label)}\")\n",
    "print(f\"Score mit Testdaten aus dem alten Dataset: {RndFrst.score(spam2_features, df.label)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam      2567\n",
       "normal    1121\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal    4788\n",
       "spam       747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(df_train.iloc[0:3500].text).toarray()\n",
    "labels = df_train.iloc[0:3500].label.factorize()[0]\n",
    "valid_features = cv.transform(df_valid.iloc[0:3500].text).toarray()\n",
    "valid_labels = df_valid.iloc[0:3500].label.factorize()[0]\n",
    "test_features = cv.transform(df_test.iloc[0:3500].text).toarray()\n",
    "test_labels = df_test.iloc[0:3500].label.factorize()[0]\n",
    "gpt_features = cv.transform(df_gpt.iloc[0:3500].text).toarray()\n",
    "gpt_labels = df_gpt.iloc[0:3500].label.factorize()[0]\n",
    "df_features = cv.transform(df.iloc[0:3500].text).toarray()\n",
    "df_labels = df.iloc[0:3500].label.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the data\n",
    "data = pd.concat([df_train, df_test, df_valid, df, df_gpt])\n",
    "\n",
    "# Mix data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split data into train, validation and test\n",
    "data_train, data_test = train_test_split(data, test_size=0.1)\n",
    "data_train, data_valid = train_test_split(data_train, test_size=0.5)\n",
    "\n",
    "# Vectorize the data\n",
    "cv = CountVectorizer()\n",
    "cv.fit(data.text)\n",
    "features = cv.transform(data_train.text).toarray()\n",
    "labels = data_train.label.factorize()[0]\n",
    "test_features = cv.transform(data_test.text).toarray()\n",
    "test_labels = data_test.label.factorize()[0]\n",
    "valid_features = cv.transform(data_valid.text).toarray()\n",
    "valid_labels = data_valid.label.factorize()[0]\n",
    "\n",
    "# Cut training and validation data to the same length\n",
    "samples = features.shape[0] - 10\n",
    "features = features[:samples]\n",
    "labels = labels[:samples]\n",
    "valid_features = valid_features[:samples]\n",
    "valid_labels = valid_labels[:samples]\n",
    "\n",
    "# test old data \n",
    "df_features = cv.transform(df.text).toarray()\n",
    "df_labels = df.label.factorize()[0]\n",
    "gpt_features = cv.transform(df_gpt.text).toarray()\n",
    "gpt_labels = df_gpt.label.factorize()[0]\n",
    "old_test_features = cv.transform(df_test.text).toarray()\n",
    "old_test_labels = df_test.label.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(features, labels, valid_features, valid_labels):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=(features.shape[1],))\n",
    "\n",
    "    # Define the hidden layers\n",
    "    hidden_layer = Dense(units=256, activation='relu')(input_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(units=128, activation='relu')(hidden_layer)\n",
    "\n",
    "    # Define the output layer\n",
    "    output_layer = Dense(units=1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(features, labels, validation_data=[valid_features, valid_labels], epochs=6, batch_size=32)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "214/214 [==============================] - 13s 56ms/step - loss: 0.2401 - accuracy: 0.9176 - val_loss: 14.9707 - val_accuracy: 0.0387\n",
      "Epoch 2/6\n",
      "214/214 [==============================] - 12s 58ms/step - loss: 0.0509 - accuracy: 0.9843 - val_loss: 20.7551 - val_accuracy: 0.0311\n",
      "Epoch 3/6\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 25.1690 - val_accuracy: 0.0317\n",
      "Epoch 4/6\n",
      "214/214 [==============================] - 12s 55ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 28.3842 - val_accuracy: 0.0329\n",
      "Epoch 5/6\n",
      "214/214 [==============================] - 12s 55ms/step - loss: 0.0084 - accuracy: 0.9965 - val_loss: 35.2010 - val_accuracy: 0.0383\n",
      "Epoch 6/6\n",
      "214/214 [==============================] - 12s 55ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 35.4146 - val_accuracy: 0.0351\n"
     ]
    }
   ],
   "source": [
    "model = train_network(features, labels, valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010283359326422215, 0.9964285492897034]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2128 - accuracy: 0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21284066140651703, 0.9791428446769714]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 6ms/step - loss: 4.8158 - accuracy: 0.1529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.8157734870910645, 0.15285713970661163]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df_features, df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 5.0668 - accuracy: 0.3684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.0667572021484375, 0.3684210479259491]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(gpt_features, gpt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(old_test_features, old_test_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5N+ShycLTgI0CJ1yYU2gG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0379c67db9d5adaac49bb272aceb784bcefbb89b24811ac05cf259bf8ea8bf96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
